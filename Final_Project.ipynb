{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fQXXeLj1w-1Y",
        "On2-rP6_bZOq",
        "f6aE1m1fxxc4",
        "qv0dInxLcLGS",
        "YL67D1Ccc_5X",
        "VstAk0CoFKmG",
        "KN998Wi1yeQw"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing 2024 – Final Project"
      ],
      "metadata": {
        "id": "N6KAEtxDv3wB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the names and ID of the submitting students here:\n",
        "\n",
        "1.Gal Ein Dor 209070671\n",
        "\n",
        "2.David Koplev 208870279\n",
        "\n",
        "3.Rotem Kashani 209073352"
      ],
      "metadata": {
        "id": "V0OgqE1-wLTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our project aims to create a smart system that can answer questions or provide relevant information from written text, we're going to use two different methods to do this: one by training a seq2seq LSTM model using a dataset found called AG News Classification Dataset, the dataset is about 4 different types of topics:\n",
        "1-World, 2-Sports, 3-Business, 4-Sci/Tech.\n",
        "\n",
        "And we aim to accurately answer questions about those topics using the model.\n",
        "The second method is by fine-tuning ChatGPT using the dataset , we believe that this method will provide better accuracy and we wish to be able to use OpenAI API models to accurately answer questions.\n"
      ],
      "metadata": {
        "id": "yVc3HY6qwfeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Step 1: Data Collection and Preprocessing\n",
        "Gather a diverse dataset of texts/article and preprocess the data by cleaning, tokenizing, and annotating questions and answers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fQXXeLj1w-1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Import Required Libraries"
      ],
      "metadata": {
        "id": "On2-rP6_bZOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "aUWZ4Ps-xTYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58aa2a56-e4ce-4b5e-84e7-8edc75942a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Load the Dataset"
      ],
      "metadata": {
        "id": "tjy3_D0nbkBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('test.csv', nrows=100)\n",
        "train_df = pd.read_csv('train.csv', nrows=1000)"
      ],
      "metadata": {
        "id": "8pWMat9JbpHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 DataProccessor Class"
      ],
      "metadata": {
        "id": "gx8KSAD91x53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def remove_stopwords(self, column, stop_words):\n",
        "        self.dataframe[column] = self.dataframe[column].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "    def lowercase(self, column):\n",
        "        self.dataframe[column] = self.dataframe[column].apply(lambda x: x.lower())\n",
        "\n",
        "    def remove_special_characters(self, column):\n",
        "        self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "    def tokenize(self, column):\n",
        "        self.dataframe[column] = self.dataframe[column].apply(word_tokenize)\n",
        "\n",
        "    def lemmatize(self, column):\n",
        "        from nltk.stem import WordNetLemmatizer\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.dataframe[column] = self.dataframe[column].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "    def preprocess(self, column, stop_words):\n",
        "        self.lowercase(column)\n",
        "        self.remove_special_characters(column)\n",
        "        self.tokenize(column)\n",
        "        self.remove_stopwords(column, stop_words)\n",
        "        self.lemmatize(column)\n",
        "\n",
        "    def get_processed_dataframe(self):\n",
        "        return self.dataframe"
      ],
      "metadata": {
        "id": "8oJK5Hx6126I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4 Get proccessed data\n"
      ],
      "metadata": {
        "id": "GM6d_oZo4QXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "processor = DataProcessor(train_df)\n",
        "processor.preprocess('Title', stop_words)\n",
        "processor.preprocess('Description', stop_words)\n",
        "\n",
        "processor = DataProcessor(test_df)\n",
        "processor.preprocess('Title', stop_words)\n",
        "processor.preprocess('Description', stop_words)"
      ],
      "metadata": {
        "id": "vKClV70E4UJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Step 2: Seq2Seq LSTM Model\n",
        "Train a seq2seq LSTM model to map questions to answers for Q&A and experiment with architectures and hyperparameters for optimal performance."
      ],
      "metadata": {
        "id": "f6aE1m1fxxc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 Import Required Libraries"
      ],
      "metadata": {
        "id": "RyTe6EBTJSFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goQcM-TpJTgm",
        "outputId": "641887c6-ba14-4fd4-a113-145e9b6eb3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Building Vocabulary"
      ],
      "metadata": {
        "id": "3Z4r9vbmJXJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(data):\n",
        "    vocab = set()\n",
        "    for _, row in data.iterrows():\n",
        "        title_tokens = row['Title']\n",
        "        desc_tokens = row['Description']\n",
        "        for token in title_tokens:\n",
        "            vocab.add(token)\n",
        "        for token in desc_tokens:\n",
        "            vocab.add(token)\n",
        "    return vocab\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab(train_df)"
      ],
      "metadata": {
        "id": "cBMVDLqeJcfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3 Mapping tokens to indices"
      ],
      "metadata": {
        "id": "8LJuQQTJDf_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map tokens to indices\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "F3vsT5PyDmCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.4 Adding a special token for unknown words"
      ],
      "metadata": {
        "id": "5QFOQtKKDnyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a special token for unknown words\n",
        "word_to_index['<UNK>'] = len(word_to_index)\n",
        "word_to_index['<SOS>'] = len(word_to_index)"
      ],
      "metadata": {
        "id": "TO3REHL4DyER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.5 Preprocessing Data\n"
      ],
      "metadata": {
        "id": "Yn8QvWHtJn8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(tokens):\n",
        "    return [word_to_index.get(token, word_to_index['<UNK>']) for token in tokens]\n",
        "\n",
        "# Apply preprocessing to data\n",
        "train_df['Title'] = train_df['Title'].apply(text_to_indices)\n",
        "train_df['Description'] = train_df['Description'].apply(text_to_indices)\n",
        "test_df['Title'] = test_df['Title'].apply(text_to_indices)\n",
        "test_df['Description'] = test_df['Description'].apply(text_to_indices)"
      ],
      "metadata": {
        "id": "39pfKIyvZBk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.6 Loading Data"
      ],
      "metadata": {
        "id": "Hs7fngCVJx05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "        # Determine maximum sequence length dynamically\n",
        "        self.max_seq_length = max(max(len(row['Title']), len(row['Description'])) for _, row in self.data.iterrows())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq = torch.tensor(self.data.iloc[idx]['Title'], dtype=torch.long)\n",
        "        target_seq = torch.tensor(self.data.iloc[idx]['Description'], dtype=torch.long)\n",
        "\n",
        "        # Pad sequences to the maximum sequence length\n",
        "        input_seq = F.pad(input_seq, (0, self.max_seq_length - len(input_seq)), value=0)\n",
        "        target_seq = F.pad(target_seq, (0, self.max_seq_length - len(target_seq)), value=0)\n",
        "\n",
        "        return input_seq, target_seq\n",
        "\n",
        "# Create DataLoader objects\n",
        "batch_size = 1\n",
        "train_dataset = QADataset(train_df)\n",
        "test_dataset = QADataset(test_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "C9Unz24yZQZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.7 Define Seq2Seq Model"
      ],
      "metadata": {
        "id": "ZJXHIiJ0J9cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "p9HE7EepZhRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.8 Training Loop"
      ],
      "metadata": {
        "id": "DFOFUwVeKVZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "size = len(vocab) + 2\n",
        "hidden_size = 256\n",
        "\n",
        "encoder = EncoderRNN(size, hidden_size)\n",
        "decoder = DecoderRNN(hidden_size, size)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Use tqdm for progress visualization\n",
        "    for input_seq, target_seq in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if input_seq.size(1) == 0:\n",
        "            continue\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_seq)\n",
        "        decoder_hidden = encoder_hidden.squeeze(0).unsqueeze(0)  # Ensure correct dimensionality\n",
        "        decoder_input = torch.tensor([[word_to_index['<SOS>']] * input_seq.size(0)], dtype=torch.long)\n",
        "        decoder_hidden = decoder_hidden.view(1, 1, -1)[:, :, :256]\n",
        "\n",
        "        loss = 0\n",
        "        for di in range(30):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output.squeeze(1), target_seq[:, di])\n",
        "            decoder_input = target_seq[:, di].unsqueeze(1)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss per batch\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {average_loss}\")"
      ],
      "metadata": {
        "id": "rIGBdujPZw3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17932c5-a2bf-4562-b072-60daedf25c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 1000/1000 [05:13<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 169.93704300308227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 1000/1000 [05:15<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 127.122130859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 1000/1000 [05:30<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 83.85162349510193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 1000/1000 [05:28<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 49.366813178539275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 1000/1000 [05:29<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 27.466068338871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 1000/1000 [05:21<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Loss: 16.358784922599792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 1000/1000 [05:27<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 11.0859784886837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 1000/1000 [05:24<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Loss: 8.33750065869093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 1000/1000 [05:27<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Loss: 6.8493596145808695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 1000/1000 [05:30<00:00,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 5.596354971945286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.9 Evaluation"
      ],
      "metadata": {
        "id": "qv0dInxLcLGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the end-of-sequence token\n",
        "EOS_token = 0  # Assign a unique integer value\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = [lang.word2index[word] for word in sentence.split()]\n",
        "    indexes.append(EOS_token)  # Add end-of-sequence token\n",
        "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()"
      ],
      "metadata": {
        "id": "w31Nb_XucNLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8b0070-01a3-4b6f-aa27-c9eeb84eb256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderRNN(\n",
              "  (embedding): Embedding(7260, 256)\n",
              "  (gru): GRU(256, 256, batch_first=True)\n",
              "  (out): Linear(in_features=256, out_features=7260, bias=True)\n",
              "  (softmax): LogSoftmax(dim=2)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.10 Regularization Model"
      ],
      "metadata": {
        "id": "YL67D1Ccc_5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the model architecture with dropout layers\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "y6o4I2g4eD1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Step 3: Fine-tuning GPT2 for Q&A\n",
        "Fine-tune GPT2 on the dataset for Q&A and implement mechanisms to quote relevant passages from the dataset."
      ],
      "metadata": {
        "id": "V7hV7ZSMyHvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Install Dependencies"
      ],
      "metadata": {
        "id": "VstAk0CoFKmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install accelerate\n",
        "!pip install pytorch_transformers"
      ],
      "metadata": {
        "id": "Y4LneVNIFQgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6eb0920-6c67-47b7-eb0c-cd996913fe3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.25.2)\n",
            "Collecting boto3 (from pytorch-transformers)\n",
            "  Downloading boto3-1.34.61-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (0.1.99)\n",
            "Collecting sacremoses (from pytorch-transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2.1.0)\n",
            "Collecting botocore<1.35.0,>=1.34.61 (from boto3->pytorch-transformers)\n",
            "  Downloading botocore-1.34.61-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.61->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.61->boto3->pytorch-transformers) (1.16.0)\n",
            "Installing collected packages: sacremoses, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.34.61 botocore-1.34.61 jmespath-1.0.1 pytorch-transformers-1.2.0 s3transfer-0.10.0 sacremoses-0.1.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.28.0\n",
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.25.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.34.61)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2.1.0)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.61 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (1.34.61)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (0.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.61->boto3->pytorch_transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch_transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch_transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.61->boto3->pytorch_transformers) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Import the necessary libraries"
      ],
      "metadata": {
        "id": "FOJYBxRSE7rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "RX-a-ljGE_HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Load GPT2 Model"
      ],
      "metadata": {
        "id": "TVU7LOFpFc9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "r1uge4uQFjxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd59ad64-319b-47f4-8d7d-c7697020c358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 665/665 [00:00<00:00, 2086172.15B/s]\n",
            "100%|██████████| 548118077/548118077 [00:16<00:00, 33744400.92B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.4 Preprocess the Dataset"
      ],
      "metadata": {
        "id": "S13tCWh8jtD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Combine Title and Description columns to form the context for the model\n",
        "train_df['context'] = train_df['Title'].astype(str) +' ' + train_df['Description'].astype(str)\n",
        "test_df['context'] = test_df['Title'].astype(str) + ' ' + test_df['Description'].astype(str)\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.data.iloc[idx]['context']\n",
        "\n",
        "          # Tokenize and convert to input IDs\n",
        "        inputs = self.tokenizer.encode(context)\n",
        "\n",
        "      # Define maximum sequence length\n",
        "        max_length = 512\n",
        "\n",
        "      # Pad or trunc ate the inputs to the maximum length\n",
        "        inputs = inputs[:max_length] if len(inputs) > max_length else inputs + [0] * (max_length - len(inputs))\n",
        "\n",
        "          # Convert input IDs to tensor\n",
        "        input_ids = torch.tensor(inputs, dtype=torch.long)\n",
        "        attention_mask = torch.ones_like(input_ids)  # Assuming no padding token is used\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': input_ids.clone()  # Labels are the same as inputs for language modeling task\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = QADataset(train_df, tokenizer)\n",
        "test_dataset = QADataset(test_df, tokenizer)\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "g_fcVpyLjx1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23bf9d39-fbe9-4e25-b5bf-0a2f9fe32371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1042301/1042301 [00:00<00:00, 3425109.26B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 1501495.20B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.5 Fine-tuning"
      ],
      "metadata": {
        "id": "-1nksibajyWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for fine-tuning\n",
        "def fine_tune_model(model, train_loader, test_loader, optimizer, scheduler, device, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Initialize the model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Initialize optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*3)\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tune_model(model, train_loader, test_loader, optimizer, scheduler, device)"
      ],
      "metadata": {
        "id": "j9EeCPdqj2Ss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5ad336-bc40-4ec5-b6bf-16e29c692886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.31816565990448\n",
            "Epoch 2/3, Loss: 0.3566182553768158\n",
            "Epoch 3/3, Loss: 0.3496329188346863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.6 Quoting Mechanism"
      ],
      "metadata": {
        "id": "byMus1RBj2vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to quote relevant passages from the text based on a query\n",
        "def quote_relevant_passage(query, text):\n",
        "    sentences = text.split('.')\n",
        "    for sentence in sentences:\n",
        "        if isinstance(query, str) and query in sentence:\n",
        "            return f'\"{sentence.strip()}.\"'\n",
        "    return None\n",
        "\n",
        "\n",
        "# Apply the quoting mechanism to the train_df\n",
        "train_df['quoted_passage'] = train_df.apply(lambda row: quote_relevant_passage(row['Description'], row['context']), axis=1)\n",
        "\n",
        "# Apply the quoting mechanism to the test_df\n",
        "test_df['quoted_passage'] = test_df.apply(lambda row: quote_relevant_passage(row['Description'], row['context']), axis=1)"
      ],
      "metadata": {
        "id": "LdHCTug3j7Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Step 4: Integration and Evaluation\n",
        "Compare the seq2seq LSTM model and ChatGPT results and evaluate the systems performances using accuracy metrics.\n"
      ],
      "metadata": {
        "id": "KN998Wi1yeQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Import the necessary libraries"
      ],
      "metadata": {
        "id": "e9SzpM0bFhd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "P7uzXHaTFoCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2 Define Evaluation Metrics"
      ],
      "metadata": {
        "id": "ggy4REThkCL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation function to calculate accuracy\n",
        "def calculate_accuracy(predictions, targets):\n",
        "    return accuracy_score(predictions, targets)\n",
        "\n",
        "# Define evaluation function for Seq2Seq LSTM model\n",
        "def evaluate_lstm_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    for batch in test_loader:\n",
        "        inputs, targets_batch = batch['input_ids'], batch['labels']\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Ensure predicted_classes is a 1-dimensional array before extending predictions\n",
        "        predicted_classes = torch.argmax(outputs).cpu().numpy()\n",
        "        if predicted_classes.ndim == 1:\n",
        "            predictions.extend(predicted_classes.tolist())\n",
        "            targets.extend(targets_batch.cpu().numpy().tolist())\n",
        "\n",
        "    return predictions, targets\n",
        "\n",
        "\n",
        "\n",
        "# Define evaluation function for ChatGPT\n",
        "def evaluate_chatgpt(model, tokenizer, test_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids)\n",
        "            logits = outputs[0]\n",
        "            predicted_ids = torch.argmax(logits, dim=-1)\n",
        "            predictions.extend(predicted_ids.flatten().cpu().numpy())\n",
        "            targets.extend(labels.flatten().cpu().numpy())\n",
        "    return predictions, targets\n",
        "\n"
      ],
      "metadata": {
        "id": "ijiZL81rkGWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.3 Evaluate Seq2Seq LSTM Model"
      ],
      "metadata": {
        "id": "gnV1143xkGye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your LSTM model class\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Seq2SeqLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Convert the input tensor x to the desired data type (e.g., torch.float32)\n",
        "      x = x.to(torch.float32)\n",
        "\n",
        "      # Pass the converted input tensor to the LSTM module\n",
        "      lstm_out, _ = self.lstm(x)\n",
        "      output = self.fc(lstm_out[-1])\n",
        "      return output\n",
        "\n",
        "# Instantiate your LSTM model\n",
        "input_dim = 10 # Example input dimension\n",
        "hidden_dim = 20 # Example hidden dimension\n",
        "output_dim = 2 # Example output dimension\n",
        "lstm_model = Seq2SeqLSTM(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Assuming you have your LSTM model and test_loader ready\n",
        "lstm_predictions, lstm_targets = evaluate_lstm_model(lstm_model, test_loader)\n",
        "\n",
        "# Assuming you have your LSTM model and test_loader ready\n",
        "lstm_accuracy = calculate_accuracy(lstm_predictions, lstm_targets)\n",
        "print(f\"Accuracy of Seq2Seq LSTM Model: {lstm_accuracy}\")\n"
      ],
      "metadata": {
        "id": "BYiKe8ZIkKh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8801ac5-23b7-4019-8271-cd357d696b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Seq2Seq LSTM Model: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.4 Evaluate ChatGPT"
      ],
      "metadata": {
        "id": "0rDb-VcxknBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_predictions, chatgpt_targets = evaluate_chatgpt(model, tokenizer, test_loader, device)\n",
        "chatgpt_accuracy = calculate_accuracy(chatgpt_predictions, chatgpt_targets)\n",
        "print(f\"Accuracy of ChatGPT: {chatgpt_accuracy}\")"
      ],
      "metadata": {
        "id": "w0jY7pZsktu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecab74e-6193-4b92-fa4a-c2a2b5a64e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of ChatGPT: 0.7953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.5 Compare Results"
      ],
      "metadata": {
        "id": "OPV2bFwbkuhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing results\n",
        "print(\"Comparison of Results:\")\n",
        "print(f\"Seq2Seq LSTM Model Accuracy: {lstm_accuracy}\")\n",
        "print(f\"ChatGPT Accuracy: {chatgpt_accuracy}\")"
      ],
      "metadata": {
        "id": "sdhcW3W5k0Mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131a4757-6b76-4b60-efa8-f4138f6da023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Results:\n",
            "Seq2Seq LSTM Model Accuracy: nan\n",
            "ChatGPT Accuracy: 0.7953125\n"
          ]
        }
      ]
    }
  ]
}