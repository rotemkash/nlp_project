{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6KAEtxDv3wB"
      },
      "source": [
        "# Natural Language Processing 2024 – Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0OgqE1-wLTJ"
      },
      "source": [
        "Add the names and ID of the submitting students here:\n",
        "\n",
        "1.Gal Ein Dor 209070671\n",
        "\n",
        "2.David Koplev 208870279\n",
        "\n",
        "3.Rotem Kashani 209073352"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVc3HY6qwfeM"
      },
      "source": [
        "Our project aims to create a smart system that can answer questions or provide relevant information from written text, we're going to use two different methods to do this: one by training a seq2seq LSTM model using a dataset found called AG News Classification Dataset, the dataset is about 4 different types of topics:\n",
        "1-World, 2-Sports, 3-Business, 4-Sci/Tech.\n",
        "\n",
        "And we aim to accurately answer questions about those topics using the model.\n",
        "The second method is by fine-tuning ChatGPT using the dataset , we believe that this method will provide better accuracy and we wish to be able to use OpenAI API models to accurately answer questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQXXeLj1w-1Y"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 1: Data Collection and Preprocessing\n",
        "Gather a diverse dataset of texts/article and preprocess the data by cleaning, tokenizing, and annotating questions and answers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On2-rP6_bZOq"
      },
      "source": [
        "##1.1 Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUWZ4Ps-xTYh",
        "outputId": "f7d126bd-51b9-42eb-cbad-9c8c54cea7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjy3_D0nbkBe"
      },
      "source": [
        "##1.2 Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "8pWMat9JbpHz"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('test.csv', nrows=500)\n",
        "train_df = pd.read_csv('train.csv', nrows=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx8KSAD91x53"
      },
      "source": [
        "##1.3 DataProccessor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "8oJK5Hx6126I"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def remove_stopwords(self, column, stop_words):\n",
        "        self.dataframe[column] = self.dataframe[column].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "    def lowercase(self, column):\n",
        "        self.dataframe[column] = self.dataframe[column].apply(lambda x: x.lower())\n",
        "\n",
        "    def remove_special_characters(self, column):\n",
        "        self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "    def tokenize(self, column):\n",
        "        self.dataframe[column] = self.dataframe[column].apply(word_tokenize)\n",
        "\n",
        "    def lemmatize(self, column):\n",
        "        from nltk.stem import WordNetLemmatizer\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.dataframe[column] = self.dataframe[column].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "    def preprocess(self, column, stop_words):\n",
        "        self.lowercase(column)\n",
        "        self.remove_special_characters(column)\n",
        "        self.tokenize(column)\n",
        "        self.remove_stopwords(column, stop_words)\n",
        "        self.lemmatize(column)\n",
        "\n",
        "    def get_processed_dataframe(self):\n",
        "        return self.dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM6d_oZo4QXv"
      },
      "source": [
        "##1.4 Get proccessed data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "vKClV70E4UJE"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "processor = DataProcessor(train_df)\n",
        "processor.preprocess('Title', stop_words)\n",
        "processor.preprocess('Description', stop_words)\n",
        "\n",
        "processor = DataProcessor(test_df)\n",
        "processor.preprocess('Title', stop_words)\n",
        "processor.preprocess('Description', stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6aE1m1fxxc4"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 2: Seq2Seq LSTM Model\n",
        "Train a seq2seq LSTM model to map questions to answers for Q&A and experiment with architectures and hyperparameters for optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyTe6EBTJSFG"
      },
      "source": [
        "##2.1 Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goQcM-TpJTgm",
        "outputId": "81da6ec8-0fa7-4a89-cd2a-70a1a3d00039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z4r9vbmJXJl"
      },
      "source": [
        "##2.2 Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "cBMVDLqeJcfP"
      },
      "outputs": [],
      "source": [
        "def build_vocab(data):\n",
        "    vocab = set()\n",
        "    for _, row in data.iterrows():\n",
        "        title_tokens = row['Title']\n",
        "        desc_tokens = row['Description']\n",
        "        for token in title_tokens:\n",
        "            vocab.add(token)\n",
        "        for token in desc_tokens:\n",
        "            vocab.add(token)\n",
        "    return vocab\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LJuQQTJDf_0"
      },
      "source": [
        "##2.3 Mapping tokens to indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "F3vsT5PyDmCW"
      },
      "outputs": [],
      "source": [
        "# Map tokens to indices\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QFOQtKKDnyV"
      },
      "source": [
        "##2.4 Adding a special token for unknown words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "TO3REHL4DyER"
      },
      "outputs": [],
      "source": [
        "# Add a special token for unknown words\n",
        "word_to_index['<UNK>'] = len(word_to_index)\n",
        "word_to_index['<SOS>'] = len(word_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn8QvWHtJn8h"
      },
      "source": [
        "##2.5 Preprocessing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "39pfKIyvZBk3"
      },
      "outputs": [],
      "source": [
        "def text_to_indices(tokens):\n",
        "    return [word_to_index.get(token, word_to_index['<UNK>']) for token in tokens]\n",
        "\n",
        "# Apply preprocessing to data\n",
        "train_df['Title'] = train_df['Title'].apply(text_to_indices)\n",
        "train_df['Description'] = train_df['Description'].apply(text_to_indices)\n",
        "test_df['Title'] = test_df['Title'].apply(text_to_indices)\n",
        "test_df['Description'] = test_df['Description'].apply(text_to_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs7fngCVJx05"
      },
      "source": [
        "##2.6 Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "C9Unz24yZQZG"
      },
      "outputs": [],
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "        # Determine maximum sequence length dynamically\n",
        "        self.max_seq_length = max(max(len(row['Title']), len(row['Description'])) for _, row in self.data.iterrows())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq = torch.tensor(self.data.iloc[idx]['Title'], dtype=torch.long)\n",
        "        target_seq = torch.tensor(self.data.iloc[idx]['Description'], dtype=torch.long)\n",
        "\n",
        "        # Pad sequences to the maximum sequence length\n",
        "        input_seq = F.pad(input_seq, (0, self.max_seq_length - len(input_seq)), value=0)\n",
        "        target_seq = F.pad(target_seq, (0, self.max_seq_length - len(target_seq)), value=0)\n",
        "\n",
        "        return input_seq, target_seq\n",
        "\n",
        "# Create DataLoader objects\n",
        "batch_size = 1\n",
        "train_dataset = QADataset(train_df)\n",
        "test_dataset = QADataset(test_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJXHIiJ0J9cx"
      },
      "source": [
        "##2.7 Define Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "p9HE7EepZhRZ"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFOFUwVeKVZJ"
      },
      "source": [
        "##2.8 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIGBdujPZw3h",
        "outputId": "2eb4cc4a-f240-47d7-a573-de0584e2c130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 606501.8876625\n",
            "Epoch 2/10, Loss: 587243.2069242188\n",
            "Epoch 3/10, Loss: 587064.7360976563\n",
            "Epoch 4/10, Loss: 587419.8859898438\n",
            "Epoch 5/10, Loss: 587427.0299445313\n",
            "Epoch 6/10, Loss: 587515.4176414063\n",
            "Epoch 7/10, Loss: 587499.8226507813\n",
            "Epoch 8/10, Loss: 587728.534678125\n",
            "Epoch 9/10, Loss: 588079.9186210937\n",
            "Epoch 10/10, Loss: 587885.429925\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define your LSTM model class\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Seq2SeqLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        # Assuming x has shape (batch_size, sequence_length, input_size)\n",
        "        if x.dim() == 2:  # If input is missing the batch dimension\n",
        "            x = x.unsqueeze(0)  # Add batch dimension\n",
        "        x = x.permute(1, 0, 2)  # Swap batch_size and sequence_length dimensions\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = lstm_out[-1]  # Extract the output of the last time step\n",
        "        output = self.fc(lstm_out)\n",
        "        return output.squeeze()  # Adjust output shape to [batch_size, output_dim]\n",
        "\n",
        "def train_lstm_model(model, train_loader, criterion, optimizer, device, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Ensure targets have the same shape as outputs\n",
        "            targets = targets.view(-1)\n",
        "\n",
        "            # Convert targets to the same data type as outputs\n",
        "            targets = targets.float()  # Assuming targets are integer labels\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Define the model, optimizer, and criterion\n",
        "input_dim = 92  # Example input dimension\n",
        "hidden_dim = 18  # Example hidden dimension\n",
        "output_dim = 92  # Example output dimension (assuming 4 classes)\n",
        "lstm_model = Seq2SeqLSTM(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming you have defined the train_loader DataLoader object\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the correct device\n",
        "lstm_model.to(device)\n",
        "\n",
        "# Activate training for Seq2Seq LSTM model\n",
        "train_lstm_model(lstm_model, train_loader, criterion, optimizer_lstm, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv0dInxLcLGS"
      },
      "source": [
        "##2.9 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "w31Nb_XucNLa"
      },
      "outputs": [],
      "source": [
        "# Define the end-of-sequence token\n",
        "EOS_token = 0  # Assign a unique integer value\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = [lang.word2index[word] for word in sentence.split()]\n",
        "    indexes.append(EOS_token)  # Add end-of-sequence token\n",
        "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL67D1Ccc_5X"
      },
      "source": [
        "##2.10 Regularization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "y6o4I2g4eD1h"
      },
      "outputs": [],
      "source": [
        "# Update the model architecture with dropout layers\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7hV7ZSMyHvp"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 3: Fine-tuning GPT2 for Q&A\n",
        "Fine-tune GPT2 on the dataset for Q&A and implement mechanisms to quote relevant passages from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VstAk0CoFKmG"
      },
      "source": [
        "##3.1 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4LneVNIFQgM",
        "outputId": "e6741f66-f6fb-4336-dfc9-70f741d49319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.25.2)\n",
            "Collecting boto3 (from pytorch-transformers)\n",
            "  Downloading boto3-1.34.66-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (0.1.99)\n",
            "Collecting sacremoses (from pytorch-transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->pytorch-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.66 (from boto3->pytorch-transformers)\n",
            "  Downloading botocore-1.34.66-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.66->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.66->boto3->pytorch-transformers) (1.16.0)\n",
            "Installing collected packages: sacremoses, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.34.66 botocore-1.34.66 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pytorch-transformers-1.2.0 s3transfer-0.10.1 sacremoses-0.1.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.28.0\n",
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.25.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.34.66)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->pytorch_transformers) (12.4.99)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.66 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (1.34.66)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (0.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.66->boto3->pytorch_transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch_transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch_transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.66->boto3->pytorch_transformers) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install accelerate\n",
        "!pip install pytorch_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOJYBxRSE7rn"
      },
      "source": [
        "##3.2 Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RX-a-ljGE_HK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVU7LOFpFc9p"
      },
      "source": [
        "##3.3 Load GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1uge4uQFjxL",
        "outputId": "f56c41c1-1567-480d-e49e-1ecb10919293"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S13tCWh8jtD8"
      },
      "source": [
        "##3.4 Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "g_fcVpyLjx1H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Combine Title and Description columns to form the context for the model\n",
        "train_df['context'] = train_df['Title'].astype(str) +' ' + train_df['Description'].astype(str)\n",
        "test_df['context'] = test_df['Title'].astype(str) + ' ' + test_df['Description'].astype(str)\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.data.iloc[idx]['context']\n",
        "\n",
        "          # Tokenize and convert to input IDs\n",
        "        inputs = self.tokenizer.encode(context)\n",
        "\n",
        "      # Define maximum sequence length\n",
        "        max_length = 512\n",
        "\n",
        "      # Pad or trunc ate the inputs to the maximum length\n",
        "        inputs = inputs[:max_length] if len(inputs) > max_length else inputs + [0] * (max_length - len(inputs))\n",
        "\n",
        "          # Convert input IDs to tensor\n",
        "        input_ids = torch.tensor(inputs, dtype=torch.long)\n",
        "        attention_mask = torch.ones_like(input_ids)  # Assuming no padding token is used\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': input_ids.clone()  # Labels are the same as inputs for language modeling task\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = QADataset(train_df, tokenizer)\n",
        "test_dataset = QADataset(test_df, tokenizer)\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1nksibajyWL"
      },
      "source": [
        "##3.5 Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9EeCPdqj2Ss",
        "outputId": "11a5f9c6-b814-425e-9de4-7be4c2c3033e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.3426806926727295\n",
            "Epoch 2/3, Loss: 0.519271969795227\n",
            "Epoch 3/3, Loss: 0.5413499474525452\n"
          ]
        }
      ],
      "source": [
        "# Define a function for fine-tuning\n",
        "def fine_tune_model(model, train_loader, test_loader, optimizer, scheduler, device, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Initialize the model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Initialize optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*3)\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tune_model(model, train_loader, test_loader, optimizer, scheduler, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byMus1RBj2vK"
      },
      "source": [
        "##3.6 Quoting Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "LdHCTug3j7Hs"
      },
      "outputs": [],
      "source": [
        "# Function to quote relevant passages from the text based on a query\n",
        "def quote_relevant_passage(query, text):\n",
        "    sentences = text.split('.')\n",
        "    for sentence in sentences:\n",
        "        if isinstance(query, str) and query in sentence:\n",
        "            return f'\"{sentence.strip()}.\"'\n",
        "    return None\n",
        "\n",
        "\n",
        "# Apply the quoting mechanism to the train_df\n",
        "train_df['quoted_passage'] = train_df.apply(lambda row: quote_relevant_passage(row['Description'], row['context']), axis=1)\n",
        "\n",
        "# Apply the quoting mechanism to the test_df\n",
        "test_df['quoted_passage'] = test_df.apply(lambda row: quote_relevant_passage(row['Description'], row['context']), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN998Wi1yeQw"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 4: Integration and Evaluation\n",
        "Compare the seq2seq LSTM model and ChatGPT results and evaluate the systems performances using accuracy metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9SzpM0bFhd2"
      },
      "source": [
        "## 4.1 Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "P7uzXHaTFoCF"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggy4REThkCL0"
      },
      "source": [
        "##4.2 Define Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ijiZL81rkGWY"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(predictions, targets):\n",
        "    # Truncate predictions and targets to the minimum length\n",
        "    min_length = min(len(predictions), len(targets))\n",
        "    predictions = predictions[:min_length]\n",
        "    targets = targets[:min_length]\n",
        "\n",
        "    # Check if the lengths are now equal\n",
        "    if len(predictions) != len(targets):\n",
        "        print(\"Error: Predictions and targets still have different lengths.\")\n",
        "        return None\n",
        "\n",
        "    if len(predictions) == 0:\n",
        "        print(\"Error: Predictions and targets are empty.\")\n",
        "        return None\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_count = sum(p == t for p, t in zip(predictions, targets))\n",
        "    total_count = len(predictions)\n",
        "    accuracy = correct_count / total_count\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def evaluate_lstm_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(test_loader):\n",
        "        inputs, targets_batch = batch[\"input_ids\"], batch[\"labels\"]\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Ensure predicted_classes is a 1-dimensional array before extending predictions\n",
        "        predicted_classes = torch.argmax(outputs, dim=-1)\n",
        "        predicted_classes_flat = predicted_classes.view(-1).cpu().numpy()  # Flatten predictions\n",
        "\n",
        "        # Extend predictions and targets with batch level granularity\n",
        "        predictions.extend(predicted_classes_flat.tolist())  # Convert to list for extend operation\n",
        "        targets.extend(targets_batch.flatten().cpu().numpy().tolist())  # Convert to list for extend operation\n",
        "\n",
        "    return predictions, targets\n",
        "\n",
        "\n",
        "\n",
        "# Define evaluation function for ChatGPT\n",
        "def evaluate_chatgpt(model, tokenizer, test_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids)\n",
        "            logits = outputs[0]\n",
        "            predicted_ids = torch.argmax(logits, dim=-1)\n",
        "            predictions.extend(predicted_ids.flatten().cpu().numpy())\n",
        "            targets.extend(labels.flatten().cpu().numpy())\n",
        "    return predictions, targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnV1143xkGye"
      },
      "source": [
        "##4.3 Evaluate Seq2Seq LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYiKe8ZIkKh8",
        "outputId": "7fb7b637-7b40-4541-8d15-ac46022189bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Seq2Seq LSTM Model: 0.788\n"
          ]
        }
      ],
      "source": [
        "# Define your LSTM model class\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Seq2SeqLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Convert the input tensor x to the desired data type (e.g., torch.float32)\n",
        "      x = x.to(torch.float32)\n",
        "\n",
        "      # Pass the converted input tensor to the LSTM module\n",
        "      lstm_out, _ = self.lstm(x)\n",
        "      output = self.fc(lstm_out[-1])\n",
        "      return output\n",
        "\n",
        "# Instantiate your LSTM model\n",
        "input_dim = 10 # Example input dimension\n",
        "hidden_dim = 20 # Example hidden dimension\n",
        "output_dim = 2 # Example output dimension\n",
        "lstm_model = Seq2SeqLSTM(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Assuming you have your LSTM model and test_loader ready\n",
        "lstm_predictions, lstm_targets = evaluate_lstm_model(lstm_model, test_loader)\n",
        "\n",
        "# Assuming you have your LSTM model and test_loader ready\n",
        "lstm_accuracy = calculate_accuracy(lstm_predictions, lstm_targets)\n",
        "print(f\"Accuracy of Seq2Seq LSTM Model: {lstm_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rDb-VcxknBj"
      },
      "source": [
        "##4.4 Evaluate ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0jY7pZsktu5",
        "outputId": "c3687c45-6e77-409f-fe15-4e9e64fded92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of ChatGPT: 0.8083046875\n"
          ]
        }
      ],
      "source": [
        "chatgpt_predictions, chatgpt_targets = evaluate_chatgpt(model, tokenizer, test_loader, device)\n",
        "chatgpt_accuracy = calculate_accuracy(chatgpt_predictions, chatgpt_targets)\n",
        "print(f\"Accuracy of ChatGPT: {chatgpt_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPV2bFwbkuhl"
      },
      "source": [
        "##4.5 Compare Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "sdhcW3W5k0Mx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "2770c69e-00e2-419d-9101-f9ddc93e2d67"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracies\n",
        "lstm_accuracy = 0.85  # Replace with actual LSTM accuracy\n",
        "chatgpt_accuracy = 0.92  # Replace with actual ChatGPT accuracy\n",
        "\n",
        "# Labels\n",
        "models = ['Seq2Seq LSTM Model', 'ChatGPT']\n",
        "accuracies = [lstm_accuracy, chatgpt_accuracy]\n",
        "\n",
        "# Creating the bar plot\n",
        "plt.bar(models, accuracies, color=['blue', 'green'])\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Comparison of Model Accuracies')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.ylim(0, 1)  # Setting y-axis limits from 0 to 1\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fQXXeLj1w-1Y",
        "On2-rP6_bZOq",
        "YL67D1Ccc_5X"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}